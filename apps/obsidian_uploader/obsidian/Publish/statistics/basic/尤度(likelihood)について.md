---
created: 2025-05-10T16:43:58+09:00
updated: 2025-05-10T21:48:49+09:00
title: 尤度(likelihood)について
category: statistics
tags: [likelihood, 最尤推定]
is_completed: true
priority: 1
summary: likelihoodの概念と、それを使った最尤推定についてのメモです。
---

# はじめに

統計や機械学習、生成AIの文脈で、「尤度(ゆうど)」という言葉を見かけることがあると思います。この意味をよく理解しておくことで、生成AIの仕組みを説明する文章を読むときにも、読みやすく楽しんで読むことができるようになると思います。そこで、自分用のメモも兼ねてこちらの記事を書く事にしました。

cf.きっかけとなった記事:

<div class="bookmark">
  <a href="https://zenn.dev/asap/articles/4092ab60570b05">https://zenn.dev/asap/articles/4092ab60570b05</a>
</div>

この記事では以下のような前提で説明を進めます。

- 生成AIモデルの中身については踏み入れません。(まだ自分も勉強中です…)
- 厳密な統計の定義や内容は扱わず、イメージを掴むことを目的とします。
- 尤度は英語でlikelihoodと呼び、こちらの方が感覚的に分かりやすいと思うので、以降はlikelihoodで統一します。
- 「確率」、「正規分布(gaussianと記述します)」は既知とします。

ちなみに、likelihoodを使った「最尤推定(maximum likelihood estimation)」は、統計学では古くから用いられており、20世紀初頭にフィッシャーによって提案された統計的推定手法だそうです。シンプルで使いやすく、私も研究の中で使っていました。

# 確率変数とは

likelihoodを理解する前に、まず「確率変数」という考え方をしっかりイメージできていることが大切なので、これを説明したいと思います。

## 確率分布と確率変数

例えば、「世界中の人の身長がどのように分布しているのか」を考えてみましょう。実際に全ての人の身長を測ることはできませんし、**本当(真)の確率分布の形はわかりません。**ですが、その確率分布を「$P$」と表す事にしましょう。

もっと具体的な話をします。「世界中の人のうち、身長が170cmの人がちょうど50%だった」と仮定します。このとき、「$P(170) = 0.5$」と書くことができます。ただし、あくまで例であり、この時$P$の真の分布の形は分かっていません。この「わからない」というのが重要なポイントです。

人間の身長は色々な値を取り得ます。身長が低い人もいれば高い人もいます。これを表すために変数を用います。ここでは、ある人の身長を$X$(cm)としましょう。

この$X$は「どんな値になるかランダム(確率的)に決まる」変数です。例えば$X > 200$、つまり身長が200cmを超えている人は少ないでしょうから、その確率は小さいでしょう。このようにある確率に応じて取りうる値が変化する変数を**確率変数**と呼びます。

つまり$X$は確率変数で、その背後にある「どの確率でどの値を取るか」という確率分布が$P$というわけですね。よくこの二つを組み合わせて、$P(X)$のような書き方をします。

## 補足: 複数の確率変数

この例では一つの変数、つまり「一人の身長」について例にしましたが、複数の変数を扱うこともあります。例えば画像生成AIの文脈では、画像の各ピクセルがそれぞれ確率的に決まると考えることができます。この場合は、確率変数を「$X = (X_1, X_2, …, X_n)$」のように複数まとめて表すこともあります。

多次元で考えるのはイメージが難しくなるので、この記事では引き続き一次元の変数で説明したいと思います。

# 確率モデル

次に考えたいのは、「この確率分布$P(X)$がどんな形をしているのか？」ということです。今まで何度か強調してきましたが、真の確率分布$P(X)$は誰にも分かりません。でも、どうにかしてこの$P(X)$を知りたいです。極論、この$P(X)$が世界の真理を表しているわけです。そこで「少しのデータから、全体の分布を推定する」と言った方針を取ります。

この時に使う考え方が、**「サンプリング」**と**「モデル化」**です。

## サンプリング

世界中の全ての人の身長を測定することは難しいですが、何人か集めて身長を測ることはできそうです。実際に、一部のデータを集めることを「サンプリング」と言います。例えばm人の身長を測ったとし、その身長を**$x = (x_1, x_2, …, x_m)$**と書く事にしましょう。$x_i$が一人一人の身長データです。この集めたデータをサンプルと呼びます。ここで、$x$と小文字で書いているのは、**すでに観測された実際のデータ**であることを表しています。先ほどの大文字の$X$は、まだ観測しておらず、どんな値になるかわからない変数、つまり確率変数である事に注意してください。文脈によって記号の使い方は様々なので、どの変数が「確率変数」なのか、「サンプルのデータ」なのか意識して読むとわかりやすくなると思います。

また、これ以降で単に$x$と書いた場合、複数のデータ点をまとめて書いているんだな、と思ってください。単に表記を簡単にしているだけです。

## モデル化

次に考えたい点は「このサンプルデータ$x$が、どのような確率分布から生まれたのかを推測すること」です。真の形は分からないですが、$P(X)$の大体の形を$x$を手がかりに推定するという事です。

今回は仮にgaussianの形をしているだろうと仮定して話を進めてみましょう。全く形がわからないという場合は、また別の手法を用いますが、とりあえず簡単のためgaussianとしましょう。

gaussianを使う一つの根拠として、身長が極端に小さいや大きい人は少なく、その中間の人が多く、gaussianで十分特徴を取り入れることができそうだと考えられるからです。

このようにどのような分布、モデルを仮定するかは状況によって変わります。その選び方にも着目して文章を読むのも面白いと思います。

## パラメータと計算

gaussianをモデルとして使う場合、分布を決めるために必要なパラメータは2つあります。

- 平均: $\mu$
- 標準偏差: $\sigma$

これをまとめて$\theta = (\mu, \sigma)$と表記します。この時、仮定した確率分布を$p(x|\theta)$と書くことにします。これは「パラメータが$\theta$の時、サンプル$x$が得られる確率」を意味します。

ここで、推定する確率分布を$p$と小文字で表しています。これも、知り得ない真の確率分布$P$と区別するためです。また、ここでも単に$\theta$と書いたときは複数のパラメータを表す事に注意してください。

ここまで準備してしまえば、パラメータ$\theta$が分かっているとき(何らかの値が与えられているとき)、その確率分布から今回得たサンプルが得られる確率が求められるはずです。全てのサンプルが独立で同じ分布から得られると仮定すると、このように書くことができます。

$$
p(x | \theta) = \prod_i^m p(x_i|\theta)
$$

## 補足

上の式は「条件付き確率」の形式で書かれています。例えば$x$であることがわかっている時に、$y$が起こる確率を$p(x|y)$と書くといった形です。この場合は、「パラメータ$\theta$が分かっているとしたときに、サンプル$x$が得られる確率はいくらか？」、という意味合いで条件付き確率となっています。

# 最尤推定

ここまで来れば尤度(likelihood)の考え方もあと少しで理解できます。実は**上で定義した「パラメータ$\theta$がわかっている時にサンプル$x$が得られる確率」$p(x|\theta)$が、likelihoodの定義です。**つまり、likelihoodは頭文字のLをとって以下のように書きます。

$$
L(\theta|x) = p(x|\theta)
$$

## Likelihoodとは？

$p(x|\theta)$は、ある分布を仮定したときに、そのサンプルが得られる確率、という意味合いで定義しましたが、語弊を恐れずに言うと、likelihoodでは$x$と$\theta$の役割を逆転させて考えます。

つまり、あるパラメータ$\theta$のもとで、実際に得られたサンプル$x$がどれくらい尤もらしいかを表す関数、もう少し砕いていうと、サンプル値$x$を得るためには、パラメータ$\theta$がどのような値であれば一番尤もらしいか、つまり、パラメータ$\theta$の関数として、この値を考えるというわけですね。

- $x$は観測されたデータで固定されている
- $\theta$は変数として色々変えてみる

元々の$p(x|\theta)$は「$\theta$が決まっている時に$x$が得られる確率」でしたが、likelihoodは$x$が固定されていて、$\theta$の関数として考える、ということです。

さらに噛み砕くと、「得られたデータ$x$を一番よく表せるパラメータ$\theta$は何か？」という考え方です。

## 最尤推定

どのパラメータが固定で、何を変化させるのかをわかりやすくするために、likelihoodの定義として$L(\theta|x)$のように書きました。この時、最尤推定とは、「likelihoodを最大にするパラメータ$\theta$を探す」という手法で、数式で表すと以下のように書けます。

$$
\hat\theta = \argmax_\theta L(\theta|x) = \argmax_\theta p(x|\theta)
$$

この尤もらしい推定値$\hat\theta$を使った分布$p$が、真の分布$P(X)$を一番再現しているだろう、とモデル化することができるということです。

## 補足: Log Likelihood

サンプル数が多いとlikelihoodは[0, 1]の値である確率をかけ合わせたものになり、値が小さくなり、最適化する際の数値計算が難しくなります。また、掛け算であるという性質を生かして、計算上の工夫としてlogを取ることをよく行います。つまり、

$$
\log L(\theta | x) = \log \prod_i^m p(x_i|\theta) = \sum_i^m \log p(x_i|\theta)
$$

として、このlog likelihoodを最大化する方法がよく使われます。よく使われるgaussianなどexp[]の形をしているので、logとの相性も良いです。

# まとめ

確率変数について、またlikelihoodと最尤推定について簡単に述べましたが、具体的なイメージが湧いたら嬉しいです。(おそらく)最尤推定は古くから用いられてきて、またシンプルな考え方なので、理解するのは難しくないかと思いますが、

- どのようなモデルが良いか？
- パラメータが多い場合の最適化問題

と言った部分が難しい問題となっています。(最適化問題も非常に面白いです…!)その部分が生成AIモデルの重要な部分なのではないかなと思うので、その前提部分の理解の助けになると嬉しいです。
